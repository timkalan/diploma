\documentclass[12pt,a4paper]{amsart}
% ukazi za delo s slovenscino -- izberi kodiranje, ki ti ustreza
\usepackage[slovene]{babel}
%\usepackage[cp1250]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{url}
%\usepackage[normalem]{ulem}
\usepackage[dvipsnames,usenames]{color}
\usepackage{pgf}
\usepackage{tikz}
\usepackage{graphicx}

\tikzstyle{block} = [rectangle, draw, 
    text width=8em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex]

% ne spreminjaj podatkov, ki vplivajo na obliko strani
\textwidth 15cm
\textheight 24cm
\oddsidemargin.5cm
\evensidemargin.5cm
\topmargin-5mm
\addtolength{\footskip}{10pt}
\pagestyle{plain}
\overfullrule=15pt % oznaci predlogo vrstico


% ukazi za matematicna okolja
\theoremstyle{definition} % tekst napisan pokoncno
\newtheorem{definicija}{Definicija}[section]
\newtheorem{primer}[definicija]{Primer}
\newtheorem{opomba}[definicija]{Opomba}

\renewcommand\endprimer{\hfill$\diamondsuit$}


\theoremstyle{plain} % tekst napisan posevno
\newtheorem{lema}[definicija]{Lema}
\newtheorem{izrek}[definicija]{Izrek}
\newtheorem{trditev}[definicija]{Trditev}
\newtheorem{posledica}[definicija]{Posledica}
\newtheorem{zgled}[definicija]{Zgled}


% za stevilske mnozice uporabi naslednje simbole
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}
\newcommand{\Q}{\mathbb Q}

% ukaz za slovarsko geslo
\newlength{\odstavek}
\setlength{\odstavek}{\parindent}
\newcommand{\geslo}[2]{\noindent\textbf{#1}\hspace*{3mm}\hangindent=\parindent\hangafter=1 #2}

% naslednje ukaze ustrezno popravi
\newcommand{\program}{Finančna matematika} % ime studijskega programa: Matematika/Finan"cna matematika
\newcommand{\imeavtorja}{Tim Kalan} % ime avtorja
\newcommand{\imementorja}{izr.~prof.~dr. Marjetka Knez} % akademski naziv in ime mentorja
\newcommand{\naslovdela}{Spodbujevalno učenje pri igranju namiznih iger}
\newcommand{\letnica}{2021} %letnica diplome


% vstavi svoje definicije ...




\begin{document}

% od tod do povzetka ne spreminjaj nicesar
\thispagestyle{empty}
\noindent{\large
UNIVERZA V LJUBLJANI\\[1mm]
FAKULTETA ZA MATEMATIKO IN FIZIKO\\[5mm]
\program\ -- 1.~stopnja}
\vfill

\begin{center}{\large
\imeavtorja\\[2mm]
{\bf \naslovdela}\\[10mm]
Delo diplomskega seminarja\\[1cm]
Mentorica: \imementorja}
\end{center}
\vfill

\noindent{\large
Ljubljana, \letnica}
\pagebreak

\thispagestyle{empty}
\tableofcontents
\pagebreak

\thispagestyle{empty}
\begin{center}
{\bf \naslovdela}\\[3mm]
{\sc Povzetek}
\end{center}
% tekst povzetka v slovenscini
V povzetku na kratko opišite vsebinske rezultate dela. Sem ne sodi razlaga organizacije
dela -- v katerem poglavju/razdelku je kaj, pa"c pa le opis vsebine.
\vfill
\begin{center}
{\bf Reinforcement learning in board games}\\[3mm] % prevod slovenskega naslova dela 
{\sc Abstract}
\end{center}
% tekst povzetka v anglescini
Prevod zgornjega povzetka v angle"s"cino.

\vfill\noindent
{\bf Math. Subj. Class. (2010):} navedite vsaj eno klasifikacijsko oznako -- 
        dostopne so na \url{www.ams.org/mathscinet/msc/msc2010.html}  \\[1mm]  
{\bf Ključne besede:} Spodbujevalno učenje, Markovski proces odločanja  \\[1mm]  
{\bf Keywords:} Reinforcement learning, Markov decision process
\pagebreak



% tu se zacne tekst seminarja
\section{Uvod}
%Na za"cetku prvega poglavja/razdelka (ali v samostojnem razdelku z naslovom Uvod) napi"site 
%kratek zgodovinski in matemati"cni uvod. Pojasnite motivacijo za problem, kje nastopa, kje 
%vse je bil obravnavan. Na koncu opi"site tudi organizacijo dela -- kaj je v kak"snem razdelku.
%
%"Ce se uvod naravno nadaljuje v besedilo prvega poglavja, lahko nadaljujete z besedilom v istem 
%razdelku, sicer za"cnete novega. Na za"cetku vsakega razdelka/podraz\-delka povete, "cemu se 
%bomo posvetili v nadaljevanju. Pri pisanju uporabljajte ukaze za matemati"cna okolja, med 
%formalnimi enotami dodajte vezni razlagalni tekst.
Namizne igre ljudje igramo že od prazgodovine. Na Kitajskem je bila igra Go znana kot ena 
izmed štirih umetnosti Kitajskega učenjaka poleg igranja inštrumenta s strunami, kaligrafije
in slikanja. Spremljajo nas že zelo dolgo časa, zato je naravno, da jih želimo ljudje čim
bolje igrati.

Z adventom računalnika in računalništva je bil ta problem postavljen v novi luči. Vprašanje
ni bilo več samo, kako dobro lahko človek igra igro sam, temveč tudi do kakšnega nivoja 
lahko spravi računalnik. Izkazalo se je, da nam pri tem problemu (in mnogih drugih) zelo dobro
koristi ">umetna inteligenca"< oz. metode strojnega učenja (SU). Eno izmed vej SU bomo 
predstavili v tem delu in pogledali, kako nam lahko pomaga pri igranju namiznih iger.

Ideja, da bi nek stroj igral igre ni nova, in kompleksnosti takega stroja so se zavedali ljudje
že pred obstojem računalnika. 
Za konec uvodnega dela morda zabeležimo še citat iz eseja ameriškega pisatelja in pesnika 
Edgarja Allana Poea, ki govori o mehaničnem igralcu šaha: 

\begin{quotation}
    %“If we choose to call the former 
    %[Chess-Player] a pure machine we must be prepared to admit that it is, beyond all comparison, 
    %the most wonderful of the inventions of mankind.”
    ">Če prej omenjenemu [igralcu šaha] rečemo čisti stroj, moramo biti pripravljeni priznati, da je
    zunaj vseh primerjav, najbolj čudovit izum človeštva.
\end{quotation}

\subsection{Motivacija}
Spodbujevalno učenje ima zelo lepo motivacijo, in sicer izhaja iz psihologije. Znana psihologa 
Thorndike in Skinner, sta na živalih izvajala eksperimente; postavila sta jih v neko novo 
situacijo, kjer je lahko žival naredila akcijo, ki je rezultirala v neki nagradi. Ko je bila
žival ponovno postavljena v to situacijo, je hitreje ugotovila, katero akcijo mora storiti, da
pride do nagrade.

Koncept, ki je opisan v zgornjem odstavku, se imenuje instrumentalno pogojevanje. Z njim se 
srečamo tudi ljudje; tako se namreč učijo otroci, odrasli ljudje pa se bolj zanesejo na 
logično razmišljanje. Vseeno pa je to motiviralo utemeljitelje spodbujevanega učenja

\subsection{Strojno učenje}
To relativno novo raziskovalno področje se deli na tri glavne veje:
\begin{itemize}
    \item \textbf{Nadzorovano učenje} se ukvarja s tem, kako iz nekih označenih podatkov 
            naučimo računalnik, da prepoznava razne signale (slike, govor, tekst, ...)
            in to znanje uporabi za razpoznavo novih, neoznačenih podatkov.
    \item \textbf{Nenadzorovano učenje} odstrani označevanje iz podatkov in v njih probava 
            odkriti skrite vzorce.
    \item \textbf{Spodbujevalno učenje} se ukvarja z ">učenjem iz izkušenj."< 
\end{itemize}

\subsection{Struktura naloge}
% morda to malo oštevilči
Naloga je razdeljena na štiri glavne dele. Na začetku so predstavljeni osnovni koncepti 
spodbujevalnega učenja in nekateri glavni algoritmi s tega področja. Potem se fokus obrne 
na namizne igre in ob nekaj malega teorije iger povzame osnovne koncepte, na katere naletimo
tam. V naslednjem odseku potem združimo znanje iz prejšnjih dveh in predstavimo, kako nam
teorija iger pripomore pri spodbujevalnem učenju v tem konteksu. Na koncu pa so predstavljeni 
nekateri empirični rezultati, ki so posledica zgoraj navedene teorije.

\section{Spodbujevalno učenje}
Spodbujevalno učenje se ukvarja s ti. učenjem iz interakcije oz. izkušenj. Čeprav se to na prvi 
pogled ne zdi kot računska metoda, pač pa stvar psihologije, bomo kmalu dognali, kako prevesti 
to idejo v računalniku razumljiv jezik.

\subsection{Osnovni koncepti}
V osnovi nas zanima precej preprosta stvar: kako preslikati neko opazovano situacijo v akcijo na 
tak način, da učenec, ki mu v spodbujevalnem učenju pravimo \textbf{agent}, maksimirizra neko 
numerično nagrado. Pri tem ne obstaja opazovalec, ki bi agentu povedal ali pa namignil, katere 
akcije so dobre, to mora ugotoviti sam, s poskušanjem in napakami. V tem dejstvu se skriva 
bistvena razlika med spodbujevalnim učenjem in ostalimi vejami strojnega učenja. 

Pomembna razlika tiči tudi v pomembnosti časa pri spodbujevalnem učenju. Pri drugih oblikah 
strojnega učenja se ponavadi ukvarjamo s tabelaričnimi podatki, tu pa ponavadi modeliramo nek 
dinamičen proces, zato je naravno, da je pomemben čas. Čeprav se ga da v tem kontekstu 
modelirati zvezno, je za naše namene dovolj, da ga jemljemo kot diskretne točke $t \in 
{1, \dots, T}$, kjer $T$ označuje nek končni čas (v splošnem je lahko seveda $T = \infty$).

\subsubsection{Nagrada}
Prvi pomemben koncept pri spodbujevanem učenju je nagrada. Kot smo že zgoraj omenili, to za nas 
pomeni neko numerično vrednost, pozitivno število indicira pozitivno nagrado, negativno pa 
">kazen"<. S pomočjo tega koncepta formaliziramo \textit{cilj} učenja. Edini cilj agenta je 
maksimizacija te nagrade, pri čemer je vredno omeniti, da na nagrado agent lahko vpliva samo 
s svojimi akcijami (ne more recimo spremeniti načina, na katerega dobi nagrado). 

Posebaj pomembno je na tem mestu poudariti, da akcije nimajo nujno neposredne nagrade. Le-te 
lahko pridejo v poljubnem kasnejšem časovnem obdobju. To je smiselno, če pomislimo z vidika 
namiznih iger: pri šahu ne razmišljamo samo o neposrednih akcijah, temveč razvijamo neko 
dolgoročno strategijo ki nas na koncu nagradi z zmago. 

\begin{zgled}[Križci in krožci]
    Pri tej znani otroški igri (in pri mnogo drugih namiznih igrah) modeliramo nagrado na 
    preprost način: če zmagamo, prejmemo nagrado $1$, če izgubimo pa $-1$. V vseh ostalih 
    situacijah, torej za izenačenje in po vsaki potezi, prejmemo nagrado $0$.
\end{zgled}

Zavedati se moramo tudi potencialnih omejitev oz. pomanjkljivosti takega modela. Razmislimo 
malo o:
\begin{definicija}[Hipoteza o nagradi]
    Vse cilje je mogoče opisati kot maksimizacijo neke kumulativne numerične 
    nagrade.
\end{definicija}

\begin{zgled}[Protiprimera hipotezi o nagradi]
    Problem je, da hipoteza dovoljuje samo enodimezionalnost:
    \begin{itemize}
        \item Ko kupujemo hamburger, nam je pomemben okus in cena; kaj nam več pomeni?
        \item Država želi med epidemijo ohraniti življenja in gospodarstvo; v kakšni 
                meri naj prioritizira ti dve katergoriji?
    \end{itemize}
    Na tem mestu poudarimo še, da se da tudi v takih situacijah modelirati nagrado na zgoraj 
    opisani način in da je ta koncept vseeno dovolj splošen, da zajame zelo velik razred problemov.
    
\end{zgled}

\subsubsection{Okolje}
Okolje predstavlja del našega sistema, na katerega agent nima nobenega vpliva. Funkcija okolja
je, da agentu pokaže \textbf{stanje} (angl. \textit{state}) in mu da nagrado glede na 
\textbf{akcijo}, ki jo prejme od njega. Če se ponovno osredotočimo na namizne igre, bi lahko rekli, 
da je okolje igralna plošča pri šahu \textit{in} nasprotnik  - tudi nanj namreč nimamo vpliva. 
Okolje nam služi tudi kot sodnik akcij oz. stanj. V kontekstu programa za igranje iger torej 
okolje izbira nasprotnikove akcije, odloča katero stanje pomeni zmago in dodeljuje nagrade.

\begin{zgled}[Križci in krožci]
    Okolje za nas pomeni $3\times3$ igralno polje in našega nasprotnika.
\end{zgled}

\subsubsection{Agent}
Kot smo že omenili zgoraj, je agent naš učenec. Njegov cilj je torej maksimizacija numerične 
nagrade, to težnjo pa dosega s pomočjo \textbf{strategije} (angl. \textit{policy}), ki mu pove, 
katero akcijo naj izbere v določenem stanju. Za ocenjevanje stanja, si pomaga z \textbf{vrednostno 
funkcijo} (angl. \textit{value function}). Kot ime implicira, je to funkcija, ki določa vrednosti 
stanjem (in akcijam). 

Nagrada nam pove takojšnjo vrednost stanja, vrednostna funkcija pa to vrednost gleda na dolgi
rok. Je izpeljanka nagrade, a veliko bolj primerna za maksimizacijo kot nagrada, saj upošteva,
da so tudi stanja, ki ne prinesejo takojšnje nagrade, lahko veliko vredna.

Poleg tega je v splošnem lažje učenje prek vrednostnih funkcij kot prek strategij neposredno, 
saj je ponavadi stanj mnogokrat manj kot možnih strategij agenta.

Formalno gledano:

\begin{definicija}
    Naj $R_t, S_t, A_t$ zaporedoma označujejo nagrado, stanje in akcijo ob času $t$. Definiramo 
    naslednje pojme: 
    \begin{itemize}
        \item Agentova \textbf{strategija} je takšna preslikava $\pi: S \rightarrow A$
                da velja 
                \begin{align*}
                a &= \pi(s) \text{ oz.}\\
                \pi(a | s) &= P(A_t = a~|~S_t = s). 
                \end{align*}
                Pri čemer prva formula definira \textit{deterministično} strategijo, druga pa 
                \textit{stohastično}. $a$ in $s$ sta realizaciji akcije in stanja v času $t$ 
                (to sta načeloma slučajni spremenljivki.)
        
        \item Naj bodo $R_{t+1}, ...,R_T$ nagrade, ji jih bomo prejeli od trenutka 
                $t$ do končnega časa. \textbf{Povračilo} (angl. \textit{return}) $G_t$ v splošnem 
                definiramo za $T=\infty$
                $$
                G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^\infty \gamma^k R_{t + k + 1} ,
                $$
                kjer je $\gamma \in [0,1]$ \textit{diskontni faktor}. Predstavlja dejstvo, da 
                imamo raje nagrade, ki bodo prišle prej. Formalno gledano, je cilj učenja 
                maksimizacija pričakovanega povračila

         \item Naj bo $\pi$ dana strategija agenta. \textbf{Vrednostna funkcija 
                stanja} (angl. \textit{state value function}) glede na strategijo $\pi$ je
                $$
                v_\pi(s) = \mathrm{E} [G_t~|~S_t = s].
                $$
                Predstavlja torej pričakovani izplen, če se vedemo skladno s strategijo $\pi$.

        \item Naj bo $\pi$ še vedno dana strategija agenta. \textbf{Vrednostna funkcija 
                akcije} (tudi stanja-akcije) (angl. \textit{action value function}) glede na 
                strategijo $\pi$ definiramo
                $$
                q_\pi(s, a) = \mathrm{E} [G_t~|~S_t = s, A_t = a].
                $$
                Pove nam pričakovani izplen, če ob času $t$ naredimo akcijo $a$, nato pa se 
                vedemo skladno s strategijo $\pi$.
    \end{itemize}
\end{definicija}

\begin{zgled}[Križci in krožci]
    Agent je v tem primeru računalniški program, ki prejme igralno ploščo, nasprotnikove 
    poteze in nagrade, vrne pa optimalno strategijo (to si želimo).
\end{zgled}

\subsubsection{Model}
Model je nenujen del našega sistema. Predstavlja znanje, ki ga ima agent o svojem okolju. 
Če imamo model, da lahko uporabimo, da napovemo, kako se bo vedlo okolje in s tem premaknemo
agentovo učenje iz čistih poskusov in napak na \textit{načrtovanje} (angl. \textit{planning}).
Model je torej poleg strategije in vrednostne funkcije še tretja komponenta agenta. Na podlagi 
modela lahko agent ">preračuna"< smiselnost svojih akcij, brez da bi dejansko karkoli storil. 

Prisotnost modela je glavna ločnica med dvema velikima, a zelo različnima vejama spodbujevalnega
učenja.

\subsection{Korak spodbujevalnega učenja}
Spodbujevalno učenje se pogosto ukvarja s procesi, ki naravno razpadejo v ti .\textbf{epizode}. 
Tak proces so recimo namizne igre, kjer so epizode precej naravno posamezne igre. Ni pa nujno, 
da je delitev tako naravna (ali pa sploh možna oz. smiselna). Za namene te diplomske naloge lahko 
privzamemo, da taka delitev obstaja.

Ideja učenja je, da agenta spustimo v okolje in mu dovolimo, da doživi (igra) mnogo epizod. Nato 
na nek način (bo razjasnjeno kasneje) ob nekih določenih časih (npr. po koncu epizode) posodobi 
svojo strategijo (in/ali vrednostno funkcijo). 

Dejanski korak (npr. poteza v namizni igri) v epizodi pa formalno gledano opredelimo: 
\begin{itemize}
    \item Agent naredi akcijo $A_t$ ob prejetem stanju $S_t$ in prejme nagrado $R_t$.
    \item Okolje prejme akcijo $A_t$, posreduje agentu stanje $S_{t+1}$ in nagrado $R_{t+1}$
\end{itemize}

\medspace

\begin{tikzpicture}[node distance = 6em, auto, thick]
    \node [block] (Agent) {Agent};
    \node [block, below of=Agent] (Okolje) {Okolje};

    \path [line] (Agent.0) --++ (4em,0em) |- node [near start]{Akcija $A_t$} (Okolje.0);
    \path [line] (Okolje.190) --++ (-6em,0em) |- node [near start] {Novo stanje  $S_{t+1}$} (Agent.170);
    \path [line] (Okolje.170) --++ (-4.25em,0em) |- node [near start, right] {Nagrada $R_{t+1}$} (Agent.190);
\end{tikzpicture}

\subsubsection{Raziskovanje in izkoriščanje}
Eden izmed glavnih problemov, s katerim se srečamo pri spodbujevalnem učenju je problem 
raziskovanja in izkoriščanja. Ko se agent uči, začne dojemati katere akcije ali pa kombinacije 
akcij mu pripeljejo nagrado. Ko to ugotovi, seveda lahko začne te akcije \textit{izkoriščati} 
in prejemativso nagrado, jim pripada. Pri tem pa naletimo na problem. Agent lahko izkorišča te 
akcije in nikoli ne ugotovi, da neka druga akcija prinese še višjo nagrado; tega ne izve, ker ne
\textit{raziskuje}. Če pa samo raziskuje pa nikoli ne izkoristi potencialnih nagrad, ki jih sreča
to je, ničesar se ne nauči. 

Uravnoteženje raziskovanja in izkoriščanja je pomemben problem, a se izkaže, da ima dokaj enostavno
rešitev (ki deluje dovolj dobro). Spoznali jo bomo v kratkem.

Morda se nekaterim bralcem zdi, da smo zaenkrat preceč ">mahali z rokami"<, to je zato, ker želimo, 
da se do te točke razvije intuicija o predstavljenih pojmih. V nadaljevanju bomo do sedaj opisane 
stvari bolj formalizirali.


\subsection{Markovski proces odločanja}
Spomnimo se najprej procesa spodbujevalnega učenja in ga poskusimo opisati bolj formalno: 
imamo zaporedje časovnih korakov $t = 0, 1, 2, \dots$, ob katerih med sabo interaktirata agent 
in okolje. Ob koraku $t$ agent prejme od okolja stanje (oz. reprezentacijo stanja) $S_t \in 
\mathcal{S}$, kjer $\mathcal{S}$ označuje množico vseh stanj. Na podalagi stanja in strategije, 
ki jo ima, izbere akcijo $A_t \in \mathcal{A}(S_t)$, kjer $\mathcal{A}(S_t)$ predstavlja 
množico akcij, ki jih ima agent na voljo v stanju $S_t$. Rezultat te akcije je nagrada $R_{t+1} 
\in \mathcal{R}$ in novo stanje $S_{t+1}$.

Čeprav se da vse opisane koncepte posplošiti na števne in celo neštevne množice stanj in akcij, 
se bomo mi omejili na končne množice. To je glede na problem, s katerim se ukvarajmo, dovolj.

\subsubsection{Markovska veriga}
Dogajanje pri spodbujevalnem učenju lahko v grobem opišemo s slučajim procesom stanj 
$(S_t)_{t=0}^T$. Zato je pomembno, da si natančno pogledamo nekaj lastnosti, ki jih 
lahko pričakujemo. 

\begin{definicija}[Markovska veriga]
    Slučajni proces $(S_t)_{t=0}^T$ na končnem verjetnostnem prostoru 
    $(\Omega, P)$ je \textbf{Markovska veriga oz. Markovski proces} (angl. \textit{Markov 
    chain}), če zanj velja Markovska lastnost
    $$
    P(S_{t+1} = s_{t+1}~|~S_{t} = s_{t}, ..., S_0 = s_0) = P(S_{t+1} = s_{t+1}~|~S_{t} = s_{t}).
    $$
    Na kratko to \textit{prehodno verjetnost} označimo $p_{ss'} := P(S_{t+1} = s'~|~S_{t} = s)$.
    Opazimo, da lahko te verjetnoti zložimo v matriko $\mathcal{P} := 
    [p_{ss'}]_{s,s'\in \mathcal{S} }$, ki ji pravimo \textit{prehodna matrika}.

    Zdaj Markovsko verigo predstavimo še na alternativni način: kot dvojico $(\mathcal{S}, 
    \mathcal{P})$, kjer je $\mathcal{P}$ zgoraj definirana prehodna matrika.
\end{definicija}

Markovska lastnost pomeni, da je prihodnost neodvisna od preteklosti, če poznamo sedanjost. 
Spodbujevalno učenje se ukvarja predvsem s problemi, kjer to dejstvo drži. Tudi pri našem 
ciljnem problemu to načeloma velja: če pogledamo igralno ploščo na katerikoli točki pogosto 
izvemo enako o trenutnem stanju, kot če bi opazovali igro od začetka.

\subsubsection{Markovski proces nagrajevanja}
Podoben koncept, ki se malo bolj približa dejanski situaciji v spodbujevalnem učenju je \textit{
Markovski proces nagrajevanja}. Kot že ime morda namigne, je precej podoben Markovski verigi, le 
da v njem nastopajo \textit{nagrade}. 

\begin{definicija}[Markovski proces nagrajevanja]
    Pričakovani nagradi glede na stanje $s$ in akcijo $a$ pravimo \textbf{nagradna funkcija} (angl.
    \textit{reward function}) in označimo
    $$
    \mathcal{R}_s^a = E[R_{t+1}~|~S_{t} = s, A_t = a].
    $$
    Naboru $(\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma)$, za katerega velja
    \begin{itemize}
        \item $\mathcal{S}$ je (končna) množica stanj,
        \item $\mathcal{P}$ je prehodna matrika $\mathcal{P}_{ss'} = P(S_{t+1} = s'~|~S_{t} = s)$, 
        \item $\mathcal{R}$ je \textit{nagradna funkcija} 
                $\mathcal{R}_s = \mathrm{E}[R_{t+1}~|~S_{t} = s]$, 
        \item $\gamma \in [0,1]$ je \textit{diskontni faktor}, 
    \end{itemize}
    pravimo \textbf{Markovski proces nagrajevanja} (angl. \textit{Markov reward process}).
\end{definicija}

Od navadne Markovske verige se torej razlikuje samo v prisotnosti nagrad ob vsakem koraku. Če te 
nagrade postavimo na $R_t = 0 \forall t$, dobimo navadno Markovsko verigo. Pomembna razlika je še 
prisotnost diskontnega faktorja $\gamma$. Če velja $\gamma < 1$, potem procesu pravimo \textit{
diskontirani Markovski proces nagrajevanja}.

Diskontiranje je motivirano iz različnih vidikov: po eni strani nam pomaga, da se v primeru 
cikličnih procesov izognemo neomejenim povračilom. Poleg tega pa je diskontiranje v mnogo pogledih 
naraven način za opis situacije: pogosto imamo raje nagrade, ki pridejo prej. Primer tega poznamo 
recimo iz ekonomije; denar, ki ga dobimo kasneje nam pomeni manj, kot tisti, ki ga dobimo takoj. 

Še vedno pa tovrsten proces ne opiše situacije v kateri se znajdemo pri spodbujevalnem učenju, saj 
ne vsebuje koncepta akcij. Je pa že dovolj ">globok"<, da v njem lahko definiramo vrednostno 
funkcijo $v(s) = \mathrm{E} [G_t~|~S_t = s]$, ki je v tem primeru neodvisna od strategije $\pi$, 
saj strategija v tem modelu nima pomena. 

Za vrednostno funkcijo lahko izpeljemo rekurzivno enačbo, s pomočjo katere lahko ">rešimo"< Markovski
proces nagrajevanja. S tem mislimo, da vsakemu stanju dodelimo pravo vrednost.

\begin{align*}
    v(s) &= \mathrm{E} [G_t~|~S_t = s] \\
         &= \mathrm{E} [\sum_{k=0}^\infty \gamma^k R_{t + k + 1}~|~S_t = s] \\
         &= \mathrm{E} [R_{t+1} + \sum_{k=1}^\infty \gamma^k R_{t + k + 1}~|~S_t = s] \\
         &= \mathrm{E} [R_{t+1} + \gamma(\sum_{k=1}^\infty \gamma^{k-1} R_{t + k + 1})~|~S_t = s] \\
         &= \mathrm{E} [R_{t+1} + \gamma G_{t+1}~|~S_t = s] \\
         &= \mathrm{E} [R_{t+1} + \gamma v(S_{t+1})~|~S_t = s]
\end{align*}

Dobili smo torej 

\begin{align}
    v(s) &= \mathrm{E} [R_{t+1} + \gamma v(S_{t+1})~|~S_t = s] \text{ oz.} \\
    v(s) &= \mathcal{R}_s + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'} v(s'),
\end{align}

kjer smo v drugi vrstici upoštevali aditivnost in idempotentnost matematičnega upanja. To je 
\textbf{Bellmanova enačba za Markovkse procese nagrajevanja}.

Ker se ukvarjamo s primerom, ko je stanj končno mnogo, recimo $n$, jih lahko brez škode za 
splošnost označimo kar $1, \dots, n$. Potem lahko Bellmanovo enačbo prepišemo v matrični obliki
\begin{align*}
    \begin{bmatrix}
        v(1) \\ 
        \vdots \\ 
        v(n)
    \end{bmatrix}
    &=
    \begin{bmatrix}
        \mathcal{R}_1 \\ 
        \vdots \\ 
        \mathcal{R}_n 
    \end{bmatrix}
    + \gamma
    \begin{bmatrix}
        \mathcal{P}_{11} \dots \mathcal{P}_{1n} \\ 
        \vdots \\ 
        \mathcal{P}_{n1} \dots \mathcal{P}_{nn}
    \end{bmatrix}
    \begin{bmatrix}
        v(1) \\ 
        \vdots \\ 
        v(n)
    \end{bmatrix} 
    \text{ ali krajše}\\
    v &= \mathcal{R} + \gamma \mathcal{P}v
\end{align*}

Opazimo, da je to \textit{linearna} enačba in da v splošnem eksplicitno rešitev: 

\begin{align*}
    v &= \mathcal{R} + \gamma \mathcal{P}v \\
    (I - \gamma \mathcal{P}) v &= \mathcal{R} \\
    v &= (I - \gamma \mathcal{P})^{-1} \mathcal{R} 
\end{align*}

Ta rešitev predpostavlja obrnljivost matrike $(I - \gamma \mathcal{P})$ in računanje njenega 
inverza, kar zahteva $O(n^3)$ operacij, zato je smiselna samo za majhne procese. Za večje 
obstajajo iterativni algoritmi in metode, nekater izmed njih bomo spoznali v naslenjem odseku.

Ta del lahko še rahlo matematiziraš - enoličnost rešitev

\subsubsection{Markovski proces odločanja}
Kot ime že nakazuje, \textit{Markovski proces odločanja (MDP)} razširja koncept procesa nagrajevanja 
z dodatkom odločanja - akcij. S tem dodatkom imamo v popolnosti opisan problem spodbujevalnega 
učenja: opisujemo torej nek proces, kjer ima agent možnost odločanja, izid pa je vsaj delno slučajen 
in odvisen od okolja. 

\begin{definicija}[Markovski proces odločanja]
    Naboru $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$, za katerega velja
    \begin{itemize}
        \item $\mathcal{S}$ je (končna) množica stanj,
        \item $\mathcal{A}$ je (končna) množica akcij, 
        \item $\mathcal{P}$ je prehodna matrika $\mathcal{P}_{ss'}^a = P(S_{t+1} = s'~|~
                S_t = s, A_t = a)$, 
        \item $\mathcal{R}$ je \textit{nagradna funkcija} 
                $\mathcal{R}_s^a = \mathrm{E}[R_{t+1}~|~S_{t} = s, A_t = a]$, 
        \item $\gamma \in [0,1]$ je \textit{diskontni faktor}, 
    \end{itemize}
    pravimo \textbf{Markovski proces odločanja} (angl. \textit{Markov decision process}).
\end{definicija}

Prehodna matrika in nagradna funkcija sta sedaj seveda odvisna tudi od akcije. V kontekstu MDP-jev
lahko sedaj formalno definiramo strategijo agenta $\pi$, ki je zahvaljujoč Markovski lastnosti 
odvisna od enega stanja in ne celotne zgodovine procesa. Definiramo tudi vrednostno funkcijo stanja 
$v_{\pi}(s)$ in vrednostno funkcijo akcije $q_{\pi}(s, a)$. Njihove definicije se ne spremenijo, so 
pa sedaj formalno umeščene v model.

Opazimo tudi, da je v MDP-ju pri fiksni strategiji $\pi$ zaporedje oz. proces stanj $S_1, S_2, 
\dots$ Markovska veriga $(\mathcal{S}, \mathcal{P}^\pi)$. Če v zaporedje stanj pomešamo še nagrade, 
torej $S_1, R_2, S_2, R_3, \dots$, dobimo Markovski proces nagrajevanja $(\mathcal{S}, 
\mathcal{P}^\pi, \mathcal{R}^\pi, \gamma)$, kjer smo označili

\begin{align*}
    \mathcal{P}^\pi &= \sum_{a \in \mathcal{A}} \pi(a|s) \mathcal{P}_{ss'}^a \\
    \mathcal{R}^\pi &= \sum_{a \in \mathcal{A}} \pi(a|s) \mathcal{R}_{s}^a. 
    \end{align*}

\begin{opomba}
    Morda je nekatere bralce zbodlo, da v zaporedju stanj in nagrad $S_1$ ne sledi $R_1$, temveč 
    $R_2$. Za tako notacijo smo se odločili, da poudarimo, da okolje agentu sočasno poda $S_2$ in 
    $R_2$, glede na stanje $S_1$ pa se agent odloča o akciji $A_1$.
\end{opomba}

MDP-ji so splošna orodja za obravnavo stohastičnih procesov, ki vključujejo odločitve v diskretnem 
času. Njihov utemeljitelj je Richard Bellman, ki je znan predvsem po izumu dinamičnega 
programiranja, zato morda ni presenetljivo, da nam prav dinamično programiranje poda osnovo za 
njihovo reševanje.
Kot pri procesih nagrajevanja, lahko tudi tu izpeljemo Bellmanovo enačbo. Najprej zapišemo 
rekurzivni zvezi za vrednostni funkciji stanjja in akcije: 

\begin{align*}
    v_\pi(s) &= \mathrm{E} [R_{t+1} + \gamma v_\pi(S_{t+1})~|~S_t = s] \\
    q_\pi(s, a) &= \mathrm{E} [R_{t+1} + \gamma q_\pi(S_{t+1})~|~S_t = s, A_t = a].
\end{align*}

Opazimo, da $v_\pi$ in $q_\pi$ povezujta zvezi:

\begin{align*}
    v_\pi(s) &= \sum_{a \in \mathcal{A}} \pi(a|s)q_\pi(s, a) \\
    q_\pi(s, a) &= \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_\pi(s').
\end{align*}

Če zvezi združimo, dobimo: 

\begin{align*}
    v_\pi(s) &= \sum_{a \in \mathcal{A}} \pi(a|s) \left[\mathcal{R}_s^a + 
    \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_\pi(s') \right] \\
    q_\pi(s, a) &= \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} 
    \mathcal{P}_{ss'}^a \left[\sum_{a' \in \mathcal{A}} \pi(a'|s')q_\pi(s', a') \right].
\end{align*}

Prvo enačbo (dej to un reference) imenujemo \textbf{Bellmanova enačba pričakovanja} (angl. 
\textit{Bellman expectation equation}), ki ima ponovno matrično obliko in eksplicitno (ENOLIČNO - 
ZAKAJ) rešitev: 

\begin{align*}
    v_\pi &= \mathcal{R}^\pi + \gamma \mathcal{P}^\pi v_\pi \\
    v_\pi &= (I - \gamma \mathcal{P}^\pi)^{-1} \mathcal{R}^\pi
\end{align*}

Ta enačba je sicer pomembna, a nas najbolj zanima ">rešitev"< MDP-ja. Želimo najti torej optimalno 
strategijo. Pri tem nam pomaga naslednje: 

\begin{definicija}
    ~
    \begin{itemize}
        \item \textbf{Optimalna vrednostna funkcija stanja} (angl. \textit{optimal state-value 
                function}) $v_*(s)$ je največja med vsemi vrednostnimi funkcijami stanj
                $$
                v_*(s) = \max_\pi v_\pi(s).
                $$
        \item \textbf{Optimalna vrednostna funkcija akcije} (angl. \textit{optimal action-value 
                function}) $q_*(s, a)$ je največja med vsemi vrednostnimi funkcijami akcij
                $$
                q_*(s, a) = \max_\pi q_\pi(s, a).
                $$
        \item Na množici vseh možnih strategij $\Pi$ definiramo delno urejenost na naslednji način:
                $$
                \pi \geq \pi', \text{če } v_\pi(s) \geq v_{\pi'}(s) \text{ } \forall s. 
                $$
    \end{itemize}
\end{definicija}

Do konca nas potem pripelje naslednji izrek. 

\begin{izrek}
    Za vsak Markovski proces odločanja velja: 
\end{izrek}

\section{Algoritmi pri spodbujevalnem učenju}
Algoritmov za reševanje MDP-jev je precej. Mi se bomo omejili predvsem na algoritme, ki se učijo 
prek vrednostne funkcije in izhajajo iz dinamičnega programiranja, a naj na tem mestu omenimo, 
da obstajajo tudi algoritmi, ki posodabljajo strategijo neposredno (mogoče kakšen reference al 
pa kej).

\subsection{Dinamično programiranje}
Je optimizacijska metoda deluje na principu deljenja velikega problema na manjše prekrivajoče 
podprobleme. V jedru reševanja je \textbf{Bellmanova enačba}, ki opisuje odnos med vrednostmi 
podproblemov in glavnega problema. Ker je idejo dinamičnega programiranja (DP) in MDP-jev dobil
Bellman ob istem času, je naravno, da je DP metoda, ki je prilagojena prav situaciji v MDP-ju.

Morda formalno bolj o bellmanovih enačbah?

\subsubsection{MRP}
Spomnimo se Markovskega procesa nagrajevanja, torej $(\mathcal{S}, \mathcal{P}, \mathcal{R}, 
\gamma)$. Na tej enostavnejši verziji MDP-ja, lahko nazorno pokažemo uporabo dinamičnega
programiranja za reševanje. Spomnimo se še vrednostne funkcije in jo poskušajmo razdeliti na
dva dela; neposredno nagrado $R_{t+1}$ in \textit{diskontirano} vrednost naslednjega stanja
$S_{t+1}$.

\subsubsection{MDP}

\subsection{Monte Carlo}
\subsection{TD(0)}
\subsection{TD($\lambda$)}

\subsection{Izboljšave}
\subsubsection{Nevronske mreže}


\section{Namizne igre}

\subsection{Pregled konceptov teorije iger}
\subsubsection{Nashevo ravnotežje}
\subsubsection{Igre z vsoto nič}
\subsubsection{Ekstenzivne igre}

\subsection{Kompleksnost iger}
\subsubsection{Game tree ...}

\subsection{Morda kaj o optimal board representationu?}
\subsection{Pride še kaj v poštev tu?}


\section{Spodbujevalno učenje pri namiznih igrah}

\subsection{Parcialni model - ">po-stanja"<}

\subsection{Učenje}
\subsubsection{Samoigra}
\subsubsection{Igre iz podatkovnih baz}
\subsubsection{Naključni nasprotnik}

\subsection{Kombinacija z iskanjem - generalised policy eval?}

\subsection{Algoritem - zaključena celota}
\subsubsection{Opomba: deluje, tudi ko ni vsota 0}


\section{Empirični rezultati}

\subsection{m,n,k-igra}
\subsubsection{Kompleksnost m,n,k-igre}

\section{Uspehi glede na velikost}
\subsubsection{Morda tudi kaj z bolj modificranimi mnk igrami}

\section{Primerjava, grafi}







%\begin{definicija}
%Funkcija $f\colon [a,b]\to\R$ je {\em zvezna}, "ce...
%\end{definicija}
%
%Osnovne rezultate o zveznih funkcijah najdemo v \cite{glob}. Navedimo le naslednji izrek.
%
%\begin{izrek}\label{izr:enakomerno}
%Zvezna funkcija na zaprtem intervalu je enakomerno zvezna.
%\end{izrek}
%
%\proof
%Na za"cetku dokaza, "ce je to le mogo"ce in smiselno, razlo"zite idejo dokaza. 
%
%Dokazovali bomo s protislovjem. Pomagali si bomo z definicijo zveznosti in s kompaktnostjo intervala.
%Izberimo $\varepsilon>0$. "Ce $f$ ni enakomerno zvezna, potem za vsak $\delta>0$ obstajata $x, y$, ki zado"s"cata
%\begin{equation}\label{eq:razlika}
%|x-y|<\delta\quad \text{in}\quad |f(x)-f(y)| \ge \varepsilon.
%\end{equation}
%\endproof
%
%Na ena"cbe se sklicujemo takole: Oglejmo si "se enkrat neena"cbi \eqref{eq:razlika}.\\
%
%"Ce dokaz trditve ne sledi neposredno formulaciji trditve, moramo povedati, kaj bomo dokazovali. To naredimo tako, da ob ukazu za izpis besede \emph{Dokaz} dodamo neobvezni parameter, v katerem napi"semo tekst, ki se bo izpisal namesto besede \emph{Dokaz}.
%
%\proof[Dokaz izreka \ref{izr:enakomerno}]
%Dokazovanja te trditve se lahko lotimo tudi takole...
%\endproof
%
%\subsection{Naslov morebitnega podrazdelka} Besedilo naj se nadaljuje v vrstici naslova, torej za ukazom \verb|\subsection{}| ne smete izpustiti prazne vrstice.
%
%V tem podrazdelku si bomo ogledali "se nekatere posledice zveznosti. 
%
%\begin{lema}
%Naj bo $f$ zvezna in ...
%\end{lema}
%
%$$\vdots$$
%
%Na konec dela sodita angle"sko-slovenski slovar"cek strokovnih izrazov in seznam uporabljene literature. Slovar naj vsebuje vse pojme, ki ste jih spoznali ob pripravi dela, pa tudi "ze znane pojme, ki ste jih spoznali pri izbirnih predmetih. Najprej navedite angle"ski pojem (ti naj bodo urejeni po abecedi) in potem ustrezni slovenski prevod; za"zeleno je, da temu sledi tudi opis pojma, lahko komentar ali pojasnilo. Slovarska gesla navajajte z ukazom \verb|\geslo{}{}|. Med zaporednima geselskima ukazoma v \LaTeX\ datoteki mora biti prazna vrstica, da so gesla izpisana vsako v svoji vrstici.
%
%Pri navajanju literature si pomagajte s spodnjimi primeri; najprej je opisano pravilo za vsak tip vira, nato so podani primeri. Posebej opozarjam, da spletni viri uporabljajo paket url, ki je vklju"cen v preambuli. Polje ``ogled'' pri spletnih virih je obvezno; "ce je kak podatek neznan, ustrezno ``polje'' seveda izpustimo. Literaturo je potrebno urediti po abecednem vrstnem redu; najprej navedemo vse vire z znanimi avtorji po abecednem redu avtorjev (po priimkih, nato imenih), nato pa spletne vire, urejene po naslovih strani. "Ce isti vir citiramo v dveh oblikah, kot tiskani in spletni vir, najprej navedemo tiskani vir, nato pa "se podatek o tem, kje je dostopen v elektronski obliki.
%
%% slovar
%\section*{Slovar strokovnih izrazov}
%
%\geslo{glide reflection}{zrcalni zdrs ali zrcalni pomik -- tip ravninske evklidske izometrije, ki je kompozitum zrcaljenja in translacije vzdol"z iste premice}
%
%\geslo{lattice}{mre"za}
%
%\geslo{link}{splet}
%
%\geslo{partition}{\textbf{$\sim$ of a set} razdelitev mno"zice; \textbf{$\sim$ of a number} raz"clenitev "stevila}
%
%
%% seznam uporabljene literature
%%\begin{thebibliography}{99}
%%
%%\bibitem{referenca-clanek}
%%I.~Priimek, \emph{Naslov "clanka}, okraj"sano ime revije \textbf{letnik revije} (leto izida) strani od--do.
%%
%%\bibitem{navodilaOMF}
%%C.~Velkovrh, \emph{Nekaj navodil avtorjem za pripravo rokopisa}, Obzornik mat.\ fiz.\ \textbf{21} (1974) 62--64.
%%
%%\bibitem{vec-avtorjev}
%%P.~Angelini, F.~Frati in M.~Kaufmann, \emph{Straight-line rectangular drawings of clustered graphs}, Discrete Comput.\ Geom.\ \textbf{45} (2011) 88--140.
%%
%%
%%
%%\bibitem{referenca-knjiga}
%%I.~Priimek, \emph{Naslov knjige}, morebitni naslov zbirke  \textbf{zaporedna "stevilka}, zalo"zba, kraj, leto izdaje.
%%
%%\bibitem{glob}
%%J.~Globevnik in M.~Brojan, \emph{Analiza I}, Matemati"cni rokopisi \textbf{25}, DMFA -- zalo"zni"stvo, Ljubljana, 2010.
%%
%%\bibitem{glob-vse}
%%J.~Globevnik in M.~Brojan, \emph{Analiza I}, Matemati"cni rokopisi \textbf{25}, DMFA -- zalo"zni"stvo, Ljubljana, 2010; dostopno tudi na
%%\url{http://www.fmf.uni-lj.si/~globevnik/skripta.pdf}.
%%
%%\bibitem{lang}
%%S.~Lang, \emph{Fundamentals of differential geometry}, Graduate Texts in Mathematics {\bf 191}, Springer-Verlag, New York, 1999.
%%
%%
%%
%%\bibitem{referenca-clanek-v-zborniku}
%%I.~Priimek, \emph{Naslov "clanka}, v: naslov zbornika (ur.\ ime urednika), morebitni naslov zbirke  \textbf{zaporedna "stevilka}, zalo"zba, kraj, leto izdaje, str.\ od--do.
%%
%%\bibitem{zbornik}
%%S.~Cappell in J.~Shaneson, \emph{An introduction to embeddings, immersions and singularities in codimension two}, v: Algebraic and geometric topology, Part 2 (ur.\ R.~Milgram), Proc.\ Sympos.\ Pure Math.\ \textbf{XXXII}, Amer.\ Math.\ Soc., Providence, 1978, str.\ 129--149.
%%
%%
%%
%%\bibitem{diploma-magisterij}
%%I.~Priimek, \emph{Naslov dela}, diplomsko/magistrsko delo, ime fakultete, ime univerze, leto.
%%
%%\bibitem{kalisnik}
%%J.~Kali"snik, \emph{Upodobitev orbiterosti}, diplomsko delo, Fakulteta za matematiko in fiziko, Univerza v Ljubljani, 2004.
%%
%%
%%
%%\bibitem{referenca-spletni-vir}
%%I.~Priimek, \emph{Naslov spletnega vira}, v: ime morebitne zbirke/zbornika, ki vsebuje vir, verzija "stevilka/datum, [ogled datum], dostopno na \url{spletni.naslov}.
%%
%%\bibitem{glob-splet}
%%J.~Globevnik in M.~Brojan, \emph{Analiza 1}, verzija 15.~9.~2010, [ogled 12.~5.~2011], dostopno na \url{http://www.fmf.uni-lj.si/~globevnik/skripta.pdf}.
%%
%%\bibitem{wiki}
%%\emph{Matrix (mathematics)}, v: Wikipedia: The Free Encyclopedia, [ogled 12.~5.~2011], dostopno na \url{http://en.wikipedia.org/wiki/Matrix_(mathematics)}.
%%
%%
%%
%%
%%\end{thebibliography}

\nocite{*}
\bibliographystyle{plain}
\bibliography{literatura}


\end{document}

